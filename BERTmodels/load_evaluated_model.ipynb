{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUh/SJQ89VaFV29DvmJTfI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GvOR0TzRweEP","executionInfo":{"status":"ok","timestamp":1692884467022,"user_tz":-180,"elapsed":16717,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}},"outputId":"953b5728-be4a-468f-b588-1b52c1e1f09e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd '/content/drive/MyDrive/Thesis project/Thesis/BERTmodels'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wsGpwG1uwsV-","executionInfo":{"status":"ok","timestamp":1692884467465,"user_tz":-180,"elapsed":449,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}},"outputId":"c2e6ff63-d337-4a4a-df57-25b84e9e25d7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Thesis project/Thesis/BERTmodels\n"]}]},{"cell_type":"code","source":["%%capture\n","!pip install pytorch-pretrained-bert\n","!pip install transformers\n","!pip install datasets\n","!pip install emoji"],"metadata":{"id":"8MyyxxA3wuIM","executionInfo":{"status":"ok","timestamp":1692884501210,"user_tz":-180,"elapsed":33747,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from  bertModel import *\n","\n","import os\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import PercentFormatter\n","\n","import pandas as pd\n","import numpy as np\n","import csv"],"metadata":{"id":"KAmjiHBkwxMw","executionInfo":{"status":"ok","timestamp":1692884510270,"user_tz":-180,"elapsed":9063,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["models_path=\"/content/drive/MyDrive/Thesis project/Thesis/BERTmodels/savedModels/selectedModels/\"\n","files_lst = os.listdir(models_path)\n","\n","\n","absolute_datasets_path=\"/content/drive/MyDrive/Thesis project/Thesis/datasets/\"\n","\n","\n","\n","#datasets=\"SARC_hot_topics_train.csv SARC_balanced_topics_train.csv\n","datasets=\"SARC_hot_topics_test.csv SARC_balanced_topics_test.csv SARC_random_test.csv preprocess_semEval2022_test.csv\""],"metadata":{"id":"Fqnajzksw0p4","executionInfo":{"status":"ok","timestamp":1692884510270,"user_tz":-180,"elapsed":9,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Loading multiple models"],"metadata":{"id":"UMTjw4ombLk3"}},{"cell_type":"code","source":["#load the pre-trained models from specific path\n","# and evaluate them to receive their results\n","\n","models_lst=[]\n","for f in files_lst:\n","  models_lst.append(models_path+f)"],"metadata":{"id":"kVyVqxLMxDEn","executionInfo":{"status":"ok","timestamp":1692884510271,"user_tz":-180,"elapsed":8,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(models_lst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uj40tM6D4IUe","executionInfo":{"status":"ok","timestamp":1692884510271,"user_tz":-180,"elapsed":7,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}},"outputId":"a83a0e96-9e74-4828-89f6-47a67514907f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/drive/MyDrive/Thesis project/Thesis/BERTmodels/savedModels/selectedModels/houmorBERT', '/content/drive/MyDrive/Thesis project/Thesis/BERTmodels/savedModels/selectedModels/toxcity_datasetBERT', '/content/drive/MyDrive/Thesis project/Thesis/BERTmodels/savedModels/selectedModels/irony_trainBERT']\n"]}]},{"cell_type":"markdown","source":["## f1 scores"],"metadata":{"id":"3k22WXy41RYm"}},{"cell_type":"code","source":["# models_names=[]\n","# for model in models_lst:\n","#   models_names.append(model.split('/')[-1])\n","\n","# rows=[]\n","\n","# header=['model name']\n","# ds_lst=datasets.split(' ')\n","# for ds in ds_lst:\n","#   header.append(ds)\n","\n","# for i in range(len(models_lst)):\n","#   b=BERTModel(model_name=models_lst[i],\n","#                   absolute_path=absolute_datasets_path,\n","#                   datasets=datasets,\n","#                   mode=\"testing\",\n","#                   print_res=False)\n","\n","#   model_name=b.get_model_name()\n","#   b.evaluate_model()\n","#   cr=b.get_classifiction_report()\n","#   keys=list(cr.keys()) # list of test datasets\n","\n","#   row=[b.get_model_name()]\n","#   for i in range(len(keys)):\n","#     res=cr[keys[i]]\n","#     test_dataset=keys[i].split('/')[-1]\n","#     try:\n","#       f1_pos=round(res['1']['f1-score'],3)\n","#     except:\n","#       f1_pos=\" \"\n","#     try:\n","#       f1_neg=round(res['0']['f1-score'],3)\n","#     except:\n","#       f1_neg= \"\"\n","\n","#     acc=round(res['accuracy'],3)\n","#     macro_f1=round(res['macro avg']['f1-score'],3)\n","#     weighted_macro=round(res['weighted avg']['f1-score'],3)\n","\n","#     row.append(f1_pos)\n","#   rows.append(row)\n","\n","# with open(\"sarcasem_detection_model_results.csv\", 'w', encoding='UTF8', newline='') as f:\n","#     writer = csv.writer(f)\n","\n","#     # write the header\n","#     writer.writerow(header)\n","\n","\n","#     #write multiple rows\n","#     writer.writerows(rows)\n"],"metadata":{"id":"uowg_YJK1Ye5","executionInfo":{"status":"ok","timestamp":1692884510660,"user_tz":-180,"elapsed":2,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Prediction mission"],"metadata":{"id":"6PVz2Zi51Lzk"}},{"cell_type":"markdown","source":["loading multiple model and for each one of the models we use him to predict every sentence and save the preditction of each sentences in table cell"],"metadata":{"id":"DCx4rsyGyq0x"}},{"cell_type":"code","source":["\n","\n","# by defualt we cheak only one dataset\n","ds=absolute_datasets_path+\"SARC_hot_topics_train.csv\"\n","#ds=absolute_datasets_path+\"SARC_60K_comments.csv\"\n","#ds=absolute_datasets_path+\"60K_RoastMe_comments.csv\"\n","\n","\n","models_names=[]\n","for model in models_lst:\n","  models_names.append(model.split('/')[-1])\n","\n","header=['text','label']\n","for model in models_names:\n","  header.append(model)\n","header.append('sum_of_probs')\n","header.append(\"product_of_probs\")\n","\n","\n","\n","\n","models_dic=dict()\n","for model in models_names:\n","  models_dic[model]=[]\n","\n","with open(\"BERTmodel_results_on_SARC_hot_topics\", 'w', encoding='UTF8', newline='') as f:\n","  writer = csv.writer(f)\n","  rows=[]\n","  data = pd.read_csv(ds)\n","  sentences=data['text']\n","  labels=data['label']\n","  # write the header\n","  writer.writerow(header)\n","  for i in range(len(models_lst)):\n","    b=BERTModel(model_name=models_lst[i],\n","                    absolute_path=absolute_datasets_path,\n","                    datasets=datasets,\n","                    mode=\"testing\",\n","                    print_res=False)\n","\n","    model_name=b.get_model_name()\n","    for i in range(len(sentences)):\n","      if (sentences[i] is not None and sentences[i]!=\"\"):\n","        res=b.predict(sentences[i])\n","        probs=res[1]\n","        pos_probs=probs[0][1]\n","        models_dic[model_name].append(pos_probs)\n","\n","  #save the results in csv file\n","  for i in range(len(sentences)):\n","    sum_probs=0\n","    prod_probs=1\n","    row=[sentences[i],labels[i]]\n","    for model in models_dic:\n","      prob=models_dic[model][i]\n","      row.append(prob)\n","      sum_probs+=prob\n","      prod_probs=prod_probs*prob\n","\n","    row.append(sum_probs)\n","    row.append(prod_probs)\n","    rows.append(row)\n","\n","  #write multiple rows\n","  writer.writerows(rows)"],"metadata":{"id":"yodVo4ypcxah","colab":{"base_uri":"https://localhost:8080/"},"outputId":"abd9d3ca-3878-420b-e138-e26a87cdaa57","executionInfo":{"status":"ok","timestamp":1692884747340,"user_tz":-180,"elapsed":236682,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]}]},{"cell_type":"markdown","source":["\n","# Load one model"],"metadata":{"id":"Pxr6lAnwbGEV"}},{"cell_type":"markdown","source":["### set the dataset path and the titles of the csv result table"],"metadata":{"id":"CqOattt7b-x7"}},{"cell_type":"markdown","source":["here we chosse one the models from the saved model list and store him at bert_model"],"metadata":{"id":"DpMd5nAk7sFS"}},{"cell_type":"code","source":["ds_lst=datasets.split(\" \")\n","full_path_datasets=[]\n","for i in range(len(ds_lst)):\n","  full_path_datasets.append(absolute_datasets_path+ds_lst[i])\n","\n","\n","header=['model name','sentence','label','probability']\n","\n","bert_model=models_lst[0]\n","print(bert_model)\n","\n","b=BERTModel(model_name=bert_model,\n","                      absolute_path=absolute_datasets_path,\n","                      datasets=datasets,\n","                      mode=\"testing\",\n","                      print_res=False\n","                      )\n","\n","b.evaluate_model()"],"metadata":{"id":"pPPxCYgObM6M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692884771225,"user_tz":-180,"elapsed":23889,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}},"outputId":"6e8f0649-5920-4b0c-f980-a9861901c07e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Thesis project/Thesis/BERTmodels/savedModels/selectedModels/houmorBERT\n","testing\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["testing\n","testing\n","testing\n"]}]},{"cell_type":"markdown","source":["### save  f1 results in csv files"],"metadata":{"id":"enWKFGhLB1A-"}},{"cell_type":"code","source":["\n","# rows=[]\n","# header=['model name','training dataset','test dataset','F1-positive class','F1- negative class','accuracy','macro avg','weighted avg']\n","\n","# cr=b.get_classifiction_report()\n","# keys=list(cr.keys()) # list of test datasets\n","\n","# with open('sarcasem_detection_model_results'+\"_\"+b.get_model_name()+\".csv\", 'w', encoding='UTF8', newline='') as f:\n","#     writer = csv.writer(f)\n","\n","#     # write the header\n","#     writer.writerow(header)\n","\n","#     for i in range(len(keys)):\n","#       res=cr[keys[i]]\n","#       test_dataset=keys[i].split('/')[-1]\n","#       try:\n","#         f1_pos=round(res['1']['f1-score'],3)\n","#       except:\n","#         f1_pos=\" \"\n","#       try:\n","#         f1_neg=round(res['0']['f1-score'],3)\n","#       except:\n","#         f1_neg= \"\"\n","\n","\n","#       acc=round(res['accuracy'],3)\n","#       macro_f1=round(res['macro avg']['f1-score'],3)\n","#       weighted_macro=round(res['weighted avg']['f1-score'],3)\n","\n","#       row=[b.get_model_name(),'',test_dataset, f1_pos,f1_neg,acc, macro_f1,weighted_macro]\n","#       rows.append(row)\n","#     #write multiple rows\n","#     writer.writerows(rows)\n"],"metadata":{"id":"l0hvvci-9PZi","executionInfo":{"status":"ok","timestamp":1692884771226,"user_tz":-180,"elapsed":32,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### print f1 results"],"metadata":{"id":"LZ8jTq3p5BKx"}},{"cell_type":"code","source":["# cr=b.get_classifiction_report()\n","# keys=list(cr.keys()) # list of test datasets\n","# for i in range(len(keys)):\n","#   res=cr[keys[i]]\n","#   test_dataset=keys[i].split('/')[-1]\n","#   f1_neg=res['0']['f1-score']\n","#   f1_pos=res['1']['f1-score']\n","#   acc=res['accuracy']\n","#   print(test_dataset)\n","#   print(round(f1_pos,3),round(f1_neg,3),round(acc,3),sep=\"\\t\")"],"metadata":{"id":"qXCyDMGu3Gc3","executionInfo":{"status":"ok","timestamp":1692884771227,"user_tz":-180,"elapsed":31,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### preditcion"],"metadata":{"id":"nw4peOwu2FyA"}},{"cell_type":"code","source":["# sent=\"Can I get ready for work in 10?Can I get ready for work in 10? #late\"\n","# res=b.predict(sent)\n","# prediction=res[0]\n","# probs=res[1]\n","# pos_probs=probs[0][1]"],"metadata":{"id":"96ROkth9xRFr","executionInfo":{"status":"ok","timestamp":1692884771227,"user_tz":-180,"elapsed":30,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### save positive and negative results in csv files"],"metadata":{"id":"woUvY2CIsT8S"}},{"cell_type":"markdown","source":["read sentences from test dataset and save the  porobility for\n","each sentence."],"metadata":{"id":"IqYaI1-PkTsW"}},{"cell_type":"markdown","source":["negative probilities"],"metadata":{"id":"AbyLBowBKOhG"}},{"cell_type":"code","source":["# for i in range(len(full_path_datasets)):\n","#   rows=[]\n","#   data = pd.read_csv(full_path_datasets[i])\n","#   sentences=data['text']\n","#   labels=data['label']\n","\n","#   with open(b.get_model_name()+\"_\"+\"negative_probilities\"+\"_\"+ds_lst[i], 'w', encoding='UTF8', newline='') as f:\n","#     writer = csv.writer(f)\n","\n","\n","#     # write the header\n","#     writer.writerow(header)\n","#     for i in range(len(sentences)):\n","#       res=b.predict(sentences[i])\n","#       probs=res[1]\n","#       neg_probs=probs[0][0]\n","#       row=[b.get_model_name(),sentences[i],labels[i],neg_probs]\n","#       rows.append(row)\n","#     #write multiple rows\n","#     writer.writerows(rows)\n"],"metadata":{"id":"D2r7ho4tkqOG","executionInfo":{"status":"ok","timestamp":1692884771228,"user_tz":-180,"elapsed":30,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["positive probilities"],"metadata":{"id":"zbwWgTVZKS_2"}},{"cell_type":"code","source":["# for i in range(len(full_path_datasets)):\n","#     rows=[]\n","#     data = pd.read_csv(full_path_datasets[i])\n","#     sentences=data['text']\n","#     labels=data['label']\n","#     with open(b.get_model_name()+\"_\"+\"positive_probilities\"+\"_\"+ds_lst[i], 'w', encoding='UTF8', newline='') as f:\n","#       writer = csv.writer(f)\n","#       # write the header\n","#       writer.writerow(header)\n","#       for i in range(len(sentences)):\n","#         res=b.predict(sentences[i])\n","#         probs=res[1]\n","#         pos_probs=probs[0][1]\n","#         # if (pos_probs>=0.8):\n","#         #   row=[b.get_model_name(),sentences[i],labels[i],pos_probs]\n","#         row=[b.get_model_name(),sentences[i],labels[i],pos_probs]\n","#         rows.append(row)\n","#       #write multiple rows\n","#       writer.writerows(rows)"],"metadata":{"id":"LLBtHIX4pWSL","executionInfo":{"status":"ok","timestamp":1692884771228,"user_tz":-180,"elapsed":30,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### plot historgrams\n","\n","plot  2 histograms of the current model for each datasets.\n","one for positive labels and the other for negative labels"],"metadata":{"id":"7ZOEWM55pOUE"}},{"cell_type":"code","source":["# # n=array_of_bins_counts\n","# def plot_values_on_top_of_bins_bars(bins,n):\n","\n","#   # x ticks is the position of the values: on the center of each one of the bins\n","#     xticks = [(bins[idx+1] + value)/2 for idx, value in enumerate(bins[:-1])]\n","\n","#     # plot values on top of bars\n","#     for idx, value in enumerate(n):\n","#         if value > 0:\n","#             #t=int(value*100)\n","#             t=round(value*100)\n","#             s=str(t) +\"%\"\n","#             plt.text(xticks[idx], value+0.001, s, ha='center')\n","\n","\n","\n","# for i in range(len(full_path_datasets)):\n","#   dataset_name=(full_path_datasets[i].split('/')[-1]).split('.')[0]\n","#   print(dataset_name)\n","\n","#   data = pd.read_csv(full_path_datasets[i])\n","#   data= data.loc[:,[\"text\",\"label\"]] # get the relveant cloumns from the dataset\n","#   data=data.sort_values('label') # sort the values by the label\n","#   data_neg_labels = data[data['label'] <=0]\n","#   data_pos_labels = data[data['label'] >=1]\n","\n","#   data_labels_lst=[data_neg_labels,data_pos_labels]\n","#   for label_idx in range(len(data_labels_lst)):# there are two labels: positive and negative\n","#     probs_lst=[]\n","#     # go over all the sentences and predict each one\n","#     for j in range(len(data_labels_lst[label_idx])):\n","#       text=data_labels_lst[label_idx].iloc[j]['text']\n","#       res=b.predict(text)\n","#       probs=res[1]\n","#       pos_probs=probs[0][1]\n","#       probs_lst.append(pos_probs)\n","\n","\n","#     plt.figure(figsize=(4,4)) #change your figure size as per your desire here\n","#     n, bins, patches =plt.hist(probs_lst, bins=[0,0.25,0.5,0.75,1],weights=np.ones(len(probs_lst)) / len(probs_lst),rwidth=0.95,alpha=0.7)\n","#     plt.xlabel('probability')\n","#     plt.ylabel('percentages')\n","#     model_name=b.get_model_name()\n","#     kind_of_labels=\"\"\n","#     if(label_idx==0):kind_of_labels=\"negative\"\n","#     else: kind_of_labels=\"positive\"\n","#     text=model_name+\" on \"+ dataset_name+ \" \"+kind_of_labels+ \" labels probs \"\n","#     #plt.title(text)\n","#     print(text)\n","#     print(\"\")\n","\n","#     plot_values_on_top_of_bins_bars(bins,n)\n","#     plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n","#     plt.show()\n","\n"],"metadata":{"id":"-lxaEDlyTNgO","executionInfo":{"status":"ok","timestamp":1692884771228,"user_tz":-180,"elapsed":30,"user":{"displayName":"David Ben-Michael","userId":"08274627946807029533"}}},"execution_count":16,"outputs":[]}]}